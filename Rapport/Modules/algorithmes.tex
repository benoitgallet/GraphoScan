\subsection{Parallélisation OpenMP}
Afin de pouvoir faire de l'acquisition vidéo en simultané sur plusieurs caméras tout en gardant le plus possible d'images par seconde, nous avons opté pour une parallélisation de l'algorithme d'acquisition grâce à OpenMP. Ainsi nous affectons - dans la mesure du possible - une caméra à un thread en lançant la région parallèle de cette sorte :
\begin{verbatim}
#pragma omp parallel num_threads(numCameras)
\end{verbatim}
Avec \texttt{numCameras} le nombre de caméras détectées. \\
Juste avant de commencer la capture, nous posons une barrière à l'aide de 
\begin{verbatim}
#pragma omp barrier
\end{verbatim}
dans le but de déclencher la capture des caméras de la manière la plus synchronisée possible.
À l'intérieur de la boucle d'acquisition, une nouvelle barrière est mise avant chaque récupération du buffer de la caméra.
Dans le cas où l'utilisateur souhaite faire un affichage de la capture qu'il est en train d'effectuer, un des threads est choisi via
\begin{verbatim}
#pragma omp single
\end{verbatim}
afin de s'occuper de cet affichage.
Enfin, chaque thread écrit la frame dans la vidéo correspondante à sa caméra.

\subsection{Export des paramètres de la caméra}

La récupération des paramètres de la caméra, calculés via MatLab, va nous permettre de faire l'undistortion et la reconstruction 3D. Jusqu'à présent, ces paramètres étaient rentrés en dur dans les programmes, nous avons donc décidé de faire des imports/exports de ces données pour plus de simplicité et de ré\-u\-ti\-li\-sa\-bi\-li\-té.
Pour ce faire, une fois les paramètres de calibration calculés dans MatLab , on récupère un objet CameraParameters par caméra. Afin de générer les fichiers de configuration nécessaires, il suffit de rentrer deux commandes par caméra :

\begin{verbatim}
	dlmwrite( 													  \
	'*PATH_TO_ACQUISITION*/Calib_camera_*NUM_CAMERA*_Matlab.txt', \
	camera*NUM_CAMERA*.IntrinsicMatrix,'delimiter', ' ',		  \
	'precision', 5)
	dlmwrite(													  \
	'*PATH_TO_ACQUISITION*/Calib_camera_*NUM_CAMERA*_Matlab.txt', \
	horzcat(camera*NUM_CAMERA*.RadialDistorsion,				  \
	camera*NUM_CAMERA*.TangentialDistorsion),					  \
	'-append', 'delimiter', ' ', 'precision',5)
\end{verbatim}

Il faut remplacer $*NUM\_CAMERA*$ par le nom de l'objet de la caméra correspondante.

\subsection{Fonctionnement de la reconstruction 3D}

Pour faire la reconstruction nous avons gardé les différentes librairies utilisées dans le code fourni. Nous avons utilisé GLFW 3 comme libraire de gestion de fenêtre et de contexte OpenGL, GLM comme librairie mathématique permettant de manipuler des matrices de toute taille et GLEW comme librairie de gestion des extensions utilisées et surtout pour gérer la plateforme sur laquelle on utilise OpenGL. \\
 
La reconstruction 3D de ce qui est écrit dans la vidéo capturée se fait en fin de programme. Le tracking produit un fichier de points 3D qui sont utlisés pour la reconstruction avec OpenGL. Ce programme s'effectue en deux phases. \\

\subsubsection{Initialisation de la fenêtre et du contexte OpenGL}
La première phase consiste à initialiser les outils OpenGL utilisés. Dans un premier temps il a fallu initialiser la fenêtre OpenGL, c'est à dire attribuer la version d'OpenGL utilisée, une taille, et même lier les fonctions d'évenement OpenGL (mouvement de souris, touches clavier, etc) à la fenêtre. Ensuite il y a une initialisation du contexte, c'est à dire créer les outils qui permettent de lier les points 3D, la fenêtre et d'autres valeurs à la carte graphique (GPU). Pour cela on utilise des Vertex Array Object (VAO) qui permettent de lier à la carte graphique un programme nommé Shader, composé de FragmentShader qui sont utilisés pour dessiner dans la fenêtre. On utilise également des Vertex Buffer Object (VBO) qui nous permettent de lier des variables (ou structures) stockées dans le processeur (CPU) avec la GPU, qui permettront d'influer sur ce qui est dessiné dans la fenêtre par le Shader. \\

\subsubsection{Dessin des points dans le repère 3D}
La deuxième phase est la phase de traitement des données. Dans un premier temps il a fallu \textit{parser} le fichier dans lequel les points 3D sont contenus. Ensuite nous avons chargé dans deux VAO différents deux Shaders, un qui sera utilisé pour construire un plan dans le repère 3D et un second qui sera utilisé pour effectuer le dessin des points 3D du tracking. Pour le dessin des points lors du \textit{parsing} nous plaçons chaque point dans des matrices à quatre lignes et une colonne qui contiendront les coordonnées du repère x,y,z et w - w permettant de retrouver les coordonnés euclidienne du point. Une fois la liste remplie nous plaçons le point dans la fenêtre OpenGL à l'aide d'un savant calcul de placement de point qu'on nommera PVM. Ce PVM n'est rien d'autre qu'une suite de multiplications de matrices ordonnées : matrice de projection * matrice de vue * matrice de modèle qui permettent de rendre visible un dessin dans une fenêtre en fonction de la position de la camera et des coordonnées du point à dessiner.
La matrice du modèle (M) est obtenue par la translation T , la rotation R appliquée sur l'objet tel que : $R * T * v = M * v$ où v est un vecteur de l'objet.
La matrice de vue (V) est obtenue par la multiplication de M et un alignement des objets de la fenêtre par rapport à la vue humaine : $v' = V * M * v$ où v est un vecteur de l'objet.
La matrice de projection (P) est obtenue par la multiplication de V, M, et une projection dans la fenêtre des objets : $v' = P * V * M * v$ où v est un vecteur de l'objet.
Une fois ces valeurs de PVM envoyées à la GPU, le point est alors dessiné et est visible sur la fenêtre.
Pour le dessin du plan nous avons repris les paramètres PVM calculés precedement, puis effectué le dessin des points placés dans le repère.
Nous avons alors en visuel le plan et ce qui a été reconstruit en 3D, une représentation de l'écriture effectuée dans la vidéo. Il est alors possible de bouger dans la fenêtre avec des touches et d'orienter sa vue en fonction de la position de la souris.